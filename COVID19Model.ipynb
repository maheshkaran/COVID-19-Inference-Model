{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The COVID-19 Model\n",
    "\n",
    "According to the method outlined in the paper \"Inferring change points in the spread of COVID-19 reveals the effectivencess of interventions\" by Dehning, Zierenberg, et. al., we use assume a standard SIR model with a new equation that accounts for the delay between _true_ infections at time _t_ and _reported_ infections at time _t_ (which we will call the reporting delay _D_) and the sinusoidal modulation of cases over a given week. In the following implementation, we only consider a stationary infection rate.\n",
    "\n",
    "We implement the PINTS model below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.chdir(\"../pints\")\n",
    "\n",
    "import pints\n",
    "\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "import math\n",
    "from scipy.stats import vonmises\n",
    "from datetime import date\n",
    "\n",
    "def SIRDiffEq (y, t, N, lam, mu):\n",
    "    #This equation describes the differential equations of the SIR model and takes in parameters:\n",
    "        #N: population size\n",
    "        #lam: lambda, the rate of disease spread\n",
    "        #mu: rate at which infected persons recover\n",
    "            \n",
    "    S, I, R = y\n",
    "            \n",
    "    dSdt = -lam * S * I / N\n",
    "    dIdt = (lam * S * I / N) - (mu * I)\n",
    "    dRdt = mu * I\n",
    "            \n",
    "    return dSdt, dIdt, dRdt\n",
    "\n",
    "class covidInferenceModel (pints.ForwardModel):\n",
    "    \n",
    "    def simulate(self, parameters, times):\n",
    "        \n",
    "        def newReportedCasesEqn (t, f_w, phi_w, delay, I0, N, lam, mu):\n",
    "            #delay between new infections and new cases of infections reported with parameters:\n",
    "                #t: array of times\n",
    "                #f_w: weekly modulation amplitude\n",
    "                #phi_w: weekly modulation phase\n",
    "                #delay: time it takes to report cases (initial cases reported when t>D)\n",
    "                #I0: number of initally infected at t=0\n",
    "                #N: population size\n",
    "                #lam: lambda, the rate of disease spread\n",
    "                #mu: rate at which infected persons recover\n",
    "                     \n",
    "            \n",
    "            #at t=0, finding infected and recovered populations, initially recovered is 0\n",
    "            casesArray = odeint(SIRDiffEq, [S0,I0,0], t, args=(N, lam, mu))\n",
    "            infectedArray = np.array(casesArray[1])\n",
    "            recoveredArray = np.array(casesArray[2])\n",
    "            \n",
    "            #number of newly infected people from t=n to t=n+1 is equal to\n",
    "            #(number of total infected from t=n to t=n+1) - (number of total recovered from t=n to t=n+1)\n",
    "            infectedCaseDifferences = np.diff(infectedArray)\n",
    "            recoveredCaseDifferences = np.diff(recoveredArray)\n",
    "            newInfectedCases = infectedCaseDifferences - recoveredCaseDifferences\n",
    "            \n",
    "            #create array of zeros of length D+1 to represent reporting delay (initial cases reported when t>D)\n",
    "            delayArray = np.zeros(D+1)\n",
    "            delayedInfectedCases = np.concatenate([delayArray,newInfectedCases])\n",
    "            delayedInfectedCases = delayedInfectedCases[0:len(t)-1:1]\n",
    "            \n",
    "            #incorporate weekly modulation\n",
    "            newReportedCases = []\n",
    "            \n",
    "            for i in range(len(t)-1):\n",
    "                newReportedCases[i] = delayInfectedCases[i]*(1-f_w)*(1-abs(math.sin(((math.pi)/7)*(t[i])-(phi_w/2))))\n",
    "            \n",
    "            return newReportedCases\n",
    "        \n",
    "    def n_parameters(self):\n",
    "        return 6\n",
    "    \n",
    "covidModel = covidInferenceModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we retrieve the data from the JHU repository over the selected time interval and construct a single output problem in PINTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array of times from t=0 to the end date\n",
    "begin_date = date(2020,3,1)\n",
    "end_date = date(2020,3,2)\n",
    "num_days = end_date - begin_date\n",
    "num_days = num_days.days+1\n",
    "\n",
    "timeStep = 1 #number of time steps per day (default is 1)\n",
    "lengthTimeArray = num_days*timeStep\n",
    "\n",
    "times = []\n",
    "for i in range(0,lengthTimeArray):\n",
    "    times.append(i)\n",
    "\n",
    "#data for Germany from 1/22/20 to 4/27/20\n",
    "confirmedInfectedTotalData = [0,0,0,0,0,1,4,4,4,5,8,10,12,12,12,12,13,13,14,14,16,16,16,16,16,16,16,16,16,16,16,16,16,16,17,27,46,48,79,130,159,196,262,482,670,799,1040,1176,1457,1908,2078,3675,4585,5795,7272,9257,12327,15320,19848,22213,24873,29056,32986,37323,43938,50871,57695,62095,66885,71808,77872,84794,91159,96092,100123,103374,107663,113296,118181,122171,124908,127854,130072,131359,134753,137698,141397,143342,145184,147065,148291,150648,153129,154999,156513,157770,158758]\n",
    "confirmedDeathsTotalData = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,3,3,7,9,11,17,24,28,44,67,84,94,123,157,206,267,342,433,533,645,775,920,1107,1275,1444,1584,1810,2016,2349,2607,2767,2736,3022,3194,3294,3804,4052,4352,4459,4586,4862,5033,5279,5575,5760,5877,5976,6126]\n",
    "confirmedRecoveredTotalData = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,12,12,12,14,14,14,14,14,15,16,16,16,16,16,16,16,16,17,18,18,18,18,25,25,46,46,46,67,67,105,113,180,233,266,266,3243,3547,5673,6658,8481,9211,13500,16100,18700,22440,24575,26400,28700,28700,36081,46300,52407,53913,57400,60300,64300,68200,72600,77000,83114,85400,88000,91500,95200,99400,103300,109800,109800,112000,114500]\n",
    "\n",
    "\n",
    "problem = pints.SingleOutputProblem(covidModel, times, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following list of parameters to be estimated:\n",
    "$$\n",
    "\\theta = [ \\lambda, \\mu, D, I_0, f_w, \\phi_w,\\sigma]\n",
    "$$\n",
    "\n",
    "where \\\\(\\sigma \\\\) is the scale width factor to the likelihood distribution. Below, we describe the likelihood for the data given the parameters, \\\\(p(\\hat C| \\theta)\\\\), as a Student-T distribution with 4 degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentTLogLikelihoodMultiplicative(pints.ProblemLogLikelihood):\n",
    "    r\"\"\"\n",
    "    Calculates a log-likelihood assuming independent Student-t-distributed\n",
    "    noise at each time point, and adds two parameters: one representing the\n",
    "    degrees of freedom (``nu``), the other representing the scale (``sigma``).\n",
    "\n",
    "    For a noise characterised by ``nu'' and ``sigma``, the log likelihood is of\n",
    "    the form:\n",
    "\n",
    "    .. math::\n",
    "        \\log{L(\\theta, \\nu, \\sigma|\\boldsymbol{x})} =\n",
    "            N\\frac{\\nu}{2}\\log(\\nu) - N\\log(\\sigma\\sqrt{f(\\theta)}) -\n",
    "            N\\log B(\\nu/2, 1/2)\n",
    "            -\\frac{1+\\nu}{2}\\sum_{i=1}^N\\log(\\nu +\n",
    "            \\frac{x_i - f(\\theta)}{\\sigma\\sqrt{f(\\theta)}}^2)\n",
    "\n",
    "    where ``B(.,.)`` is a beta function.\n",
    "\n",
    "    Extends :class:`ProblemLogLikelihood`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    problem\n",
    "        A :class:`SingleOutputProblem` or :class:`MultiOutputProblem`. For a\n",
    "        single-output problem two parameters are added ``(nu, sigma)``, where\n",
    "        ``nu`` is the degrees of freedom and ``sigma`` is scale, for a\n",
    "        multi-output problem ``2 * n_outputs`` parameters are added.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, problem):\n",
    "        super(StudentTLogLikelihoodMultiplicative, self).__init__(problem)\n",
    "\n",
    "        # Get number of times, number of outputs\n",
    "        self._nt = len(self._times)\n",
    "        self._no = problem.n_outputs()\n",
    "\n",
    "        # Add parameters to problem (two for each output)\n",
    "        self._n_parameters = problem.n_parameters() + 2 * self._no\n",
    "\n",
    "        # Pre-calculate\n",
    "        self._n = len(self._times)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # For multiparameter problems the parameters are stored as\n",
    "        # (model_params_1, model_params_2, ..., model_params_k,\n",
    "        # nu_1, sigma_1, nu_2, sigma_2,...)\n",
    "        n = self._n\n",
    "        m = 2 * self._no\n",
    "\n",
    "        # problem parameters\n",
    "        problem_parameters = x[:-m]\n",
    "        y = self._problem.evaluate(problem_parameters)\n",
    "        error = self._values - y\n",
    "\n",
    "        # Distribution parameters\n",
    "        parameters = x[-m:]\n",
    "        nu = np.asarray(parameters[0::2])\n",
    "        sigma = np.asarray(parameters[1::2])\n",
    "\n",
    "        # Calculate\n",
    "        return np.sum(\n",
    "            + 0.5 * n * nu * np.log(nu)\n",
    "            - n * np.log(sigma * np.sqrt(y))\n",
    "            - n * np.log(scipy.special.beta(0.5 * nu, 0.5))\n",
    "            - 0.5 * (1 + nu) * np.sum(np.log(nu + (error / (sigma * sqrt(y)))**2), axis=0)\n",
    "        )\n",
    "\n",
    "class StudentTLogLikelihoodMultiplicative4(pints.ProblemLogLikelihood):\n",
    "    r\"\"\"\n",
    "    Calculates a log-likelihood assuming independent Student-t-distributed\n",
    "    noise at each time point, and adds two parameters: one representing the\n",
    "    degrees of freedom (``nu``), the other representing the scale (``sigma``).\n",
    "\n",
    "    For a noise characterised by ``nu'' and ``sigma``, the log likelihood is of\n",
    "    the form:\n",
    "\n",
    "    .. math::\n",
    "        \\log{L(\\theta, \\sigma|\\boldsymbol{x})} =\n",
    "            N\\frac{4}{2}\\log(4) - N\\log(\\sigma\\sqrt{f(\\theta)}) -\n",
    "            N\\log B(4/2, 1/2)\n",
    "            -\\frac{1+4}{2}\\sum_{i=1}^N\\log(4 +\n",
    "            \\frac{x_i - f(\\theta)}{\\sigma\\sqrt{f(\\theta)}}^2)\n",
    "\n",
    "    where ``B(.,.)`` is a beta function.\n",
    "\n",
    "    Extends :class:`ProblemLogLikelihood`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    problem\n",
    "        A :class:`SingleOutputProblem` or :class:`MultiOutputProblem`. For a\n",
    "        single-output problem one parameter is added ``(sigma)``, where\n",
    "        ``sigma`` is scale, for a multi-output problem ``n_outputs`` parameters are added.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, problem):\n",
    "        super(StudentTLogLikelihoodMultiplicative4, self).__init__(problem)\n",
    "\n",
    "        # Get number of times, number of outputs\n",
    "        self._nt = len(self._times)\n",
    "        self._no = problem.n_outputs()\n",
    "\n",
    "        # Add parameters to problem (two for each output)\n",
    "        self._n_parameters = problem.n_parameters() + self._no\n",
    "\n",
    "        # Pre-calculate\n",
    "        self._n = len(self._times)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # For multiparameter problems the parameters are stored as\n",
    "        # (model_params_1, model_params_2, ..., model_params_k,\n",
    "        # nu_1, sigma_1, nu_2, sigma_2,...)\n",
    "        n = self._n\n",
    "        m = self._no\n",
    "\n",
    "        # problem parameters\n",
    "        problem_parameters = x[:-m]\n",
    "        y = self._problem.evaluate(problem_parameters)\n",
    "        error = self._values - y\n",
    "\n",
    "        # Distribution parameters\n",
    "        parameters = x[-m:]\n",
    "        sigma = np.asarray(parameters[0::2])\n",
    "        nu = np.repeat(4, m)\n",
    "\n",
    "        # Calculate\n",
    "        return np.sum(\n",
    "            + 0.5 * n * nu * np.log(nu)\n",
    "            - n * np.log(sigma * np.sqrt(y))\n",
    "            - n * np.log(scipy.special.beta(0.5 * nu, 0.5))\n",
    "            - 0.5 * (1 + nu) * np.sum(np.log(nu + (error / (sigma * sqrt(y)))**2), axis=0)\n",
    "        )\n",
    "    \n",
    "likelihood = StudentTLogLikelihoodMultiplicative4(problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the prior distribution for each of the parameters to be estimated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda_prior = pints.LogNormalLogPrior(math.log10(0.4),0.5) # lamda has Log-Normal prior\n",
    "mu_prior = pints.LogNormalLogPrior(math.log10(1/8),0.2) # mu has Log-Normal prior\n",
    "delay_prior = pints.LogNormalLogPrior(math.log10(8),0.2) # D has Log-Normal prior\n",
    "initInfected_prior = pints.HalfCauchyLogPrior(0,100) # I0 has Half-Cauchy prior\n",
    "scaleWidth_prior = pints.HalfCauchyLogPrior(0,10) # sigma has Half-Cauchy prior\n",
    "fw_prior = pints.BetaLogPrior(0.7, 0.17) # f_w has Beta prior\n",
    "phi_prior = vonmises.pdf(0,0.01) # phi_w has Von Mises prior\n",
    "\n",
    "log_priors = [lamda_prior, mu_prior, delay_prior, initInfected_prior, scaleWidth_prior, fw_prior, phi_prior]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the posterior distribution, \\\\(p(\\theta|\\hat C )\\\\), below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Given prior must extend pints.LogPrior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-a17e6038bdd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mposterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogPosterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_priors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/pints-master/pints/_log_pdfs.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, log_likelihood, log_prior)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogPrior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise ValueError(\n\u001b[0;32m--> 169\u001b[0;31m                 'Given prior must extend pints.LogPrior.')\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogPDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Given prior must extend pints.LogPrior."
     ]
    }
   ],
   "source": [
    "posterior = pints.LogPosterior(likelihood, log_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the MCMC sampling scheme, using the adaptive covariance method (Haario-Bardenet). According to the paper, we need to create 4 chains, perform 1000 burn-in steps, and sample for 4000 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startingParamVals = [initLam, initMu, initDelay, initInfected, initScale, initFw, initPhi]\n",
    "\n",
    "xs = [\n",
    "    startingParamVals * 1.1,\n",
    "    startingParamVals * 1.05,\n",
    "    startingParamVals * 0.9,\n",
    "    startingParamVals * 1.15,\n",
    "]\n",
    "\n",
    "# Create mcmc routine with four chains\n",
    "mcmc = pints.MCMCController(posterior, 4, xs, method=pints.HaarioBardenetACMC)\n",
    "\n",
    "# Add stopping criterion\n",
    "mcmc.set_max_iterations(4000)\n",
    "\n",
    "# Start adapting after 1000 iterations\n",
    "mcmc.set_initial_phase_iterations(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
